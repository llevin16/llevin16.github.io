{
  "name": "Using unsupervised learning to create 'strategies' within English Premier League soccer",
  "tagline": "Can data itself show us the styles of play in English soccer?",
  "body": "## Background\r\n\r\nCan strategy be gleaned from data? This is the question I sought to answer with this project. I chose soccer as the environment in which to pursue this as I have been a lover of soccer all my life and consider it, as many other do, an immensely strategic game. What type of passing (the long balls of Sunderland or short 'tiki taka' of Barcelona), what type of corners, how much possession, how many shots, all can be dictated by the team's predetermined strategy.\r\n\r\nTo be clear, I am not trying to predict anything. In data science, this type of project is called 'unsupervised learning', as there is no clear goal other than to learn about the data and see what it holds. I do not have a 'target' variable (the variable you want to predict using others, such as profit, or stock price, etc.). The goal of this project is to see if standard data science techniques can yield easily understandable groups of data which can be described as 'strategies'. \r\n\r\nThere were four main steps to determining this:\r\n- Data Cleaning (finding the data from whichever sources available and converting the data into something usable for modeling)\r\n- EDA (standing for Exploratory Data Analysis, which is looking at the data and seeing what changes must be made to the data to ensure final results won't be skewed by an abnormalities)\r\n- Modeling (the piece where we choose a specific model to use and apply it to the data)\r\n- Evaluation (looking at the results of the modeling piece and deciding how, if at all, it answers are initial question)\r\n\r\n## Data Cleaning\r\n\r\nThe data I used for this came from many different sources around the web. Some were databases downloaded directly (both in SQL, CSV, and JSON formats) from the internet, others were 'scraped' manually (meaning I had to create a process to find the data I needed on the web and essentially copy and paste it into my own database), and others were requested (using websites similar to downloading it directly).\r\n\r\nThe first question was what scale to look at. Should I look at individual players? A whole season? Game by game? This was relatively easy to answer as there is very limited player statistics available and anything higher level than game by game would also create too small a dataset. The next piece was to get as much game by game information as possible, with some scope constraints. To begin with I wanted to only focus on one league so I chose the English Premier League and only took games within that league. This was to make sure differences between leagues didn't create too much noise for the models to work effectively. Using Github, Kaggle, and other websites, I was able to create a dataset of over 900 games which included which stadium the game was played, the teams, and a host of other soccer specific metrics such as number of shots, possession (how much of the ball each team had during the game), passing accuracy, etc. \r\n\r\nTo get all these games into one dataset a lot of rearranging had to take place: making sure numeric columns were considered numeric, creating 'dummy' columns for categorical data (the models we will discuss later only take numerical data as inputs so categories were converted to columns with a 1 or 0 if the game had that information), merging datasets together, and other data cleaning or 'munging' techniques.\r\n\r\nAn interesting angle was to see if the weather had an effect. Do teams play different when it is hot or cold? When it's raining? To include this I downloaded over ten years of weather data from Great Britain's weather stations which had data in up to 20 minute incremenets. This data included the temperature, wind speed, visibility, etc.\r\n\r\nTo merge this data to the soccer dataset, I used the latitude and longitude coordinates of the stadiums where each game was played. I then selected the weather station that was closest to those coordinates and merged the weather associated. I also made the assumption that all soccer games started at the same time of 15:00 UTC (the exact start time wasn't available for each game and that time is the most common start time for English Premier League games).\r\n\r\nI now have a dataset of over 900 soccer games with extensive statistics on each game.\r\n\r\n## EDA\r\n\r\nBefore choosing a model and modeling seeing if any groupings can be found I had to look and visualize the data to make sure no oddities were included. The first thing I did was to check the stadiums that were included. Some initial graphs like the ony below seem to show good data.\r\n\r\nHowever, it seemed odd that a few stadiums had unusually high values. A simple check determined that these stadiums only had a few games played within the dataset, which could heavily skew the modeling. I therefore removed a few games played at these stadiums.\r\n\r\nSimilar visualizations were created to look at other fields and groupings for similar issues. This way I also found that unfortunately some weather data was not included in the dataset I downloaded and so had to remove those fields as well. \r\n\r\nOverall I was left with ~90 fields which I still believe is enough data to create some good clusterings.\r\n\r\n## Modeling\r\n\r\nNow to the modeling. The main decision is which model to use for this process. Unsupervised learning has many algorithms which can create clusters from varying amounts of data. However, one main component to my question was that I have no prior idea as to how many groups should be contained in the output. The strategies could be few or many and anywhere in between. Therefore I want to use a model which does not require setting a number of groups to output. This left me with the Density-based spatial clustering of applications with noise algorithm, known as DBSCAN. This is an exceptionally good model when working with multiple types and sizes of data, as well as accounting for noise.\r\n\r\nHowever, one piece that must be created first is something called a 'distance matrix'. This is simply a square matrix which shows how close variables are to each based on a formula. Usually the Euclidian distance can be used when all the variables are continuous. Since my dataset includes different types of data (continuous, discrete, the 'dummy' variables mentioned previously), another type of distance formula was used. This is the Gower distance which simply compares variables and applies a 'similarity coefficiant' which is between 0 and 1, 1 being identical and 0 being completely separate. \r\n\r\nThe double-edged sword of unsupervised learning is there is no clear way of determining how well your model works. This is great since there are many more options but it does requires a manual process for fine tuning the inputs and parameters of the model. For example, when deciding which group a data point falls into, you can set the maximum distance which is allowed for a point to be in a group. You can also set the minimum number of points that can be called a group. This requires trial and error to find a grouping that makes sense.\r\n\r\n## Evaluation\r\n\r\nSo how did DBSCAN do? The answer is unfortunate and inconclusive. Even with the data provided it could not create any sort of meaningful grouping which intuitively split the dataset. Either each point was it's own group or almost every point was in one group with a few outliers. This shows that either there wasn't enough data or the data included wasn't distinct enough to create separate groupings.\r\n\r\n## Conclusion\r\n\r\nOur original hypothesis was whether intuitive groupings could be created from the soccer statistics provided which could be translated into strategies. It seems with the current dataset, we cannot. DBSCAN was the best modeling option and still counldn't provide groupings above a few data outliers. \r\n\r\nHowever, this doesn't mean it isn't possible. The next steps would be to increase our data available (expand to different leagues within England to start and then outside England if necessary) and delve deeper into the fields we used. At times there can be too many fields for DBSCAN to accurately group and so looking at pairwise interactions and correlations could help us determine which variables matter the most for these groupings.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}